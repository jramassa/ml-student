{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DctEQLOxvgZw"
      },
      "source": [
        "# Vision Transformer (ViT) model for image classification\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNOX5dRZvgZ3"
      },
      "source": [
        "## Introduction\n",
        "A lot of the recent state-of-the-art models are based on transformers. For example, [image classification on ImageNet](https://paperswithcode.com/sota/image-classification-on-imagenet?dimension=Top%205%20Accuracy) benchmark is lead by architectures using transformers.\n",
        "\n",
        "This example implements the [Vision Transformer (ViT)](https://arxiv.org/abs/2010.11929) model for image classification, and demonstrates it on satellite images.\n",
        "\n",
        "Vision Transformer is a type of artificial neural network architecture that is specifically designed for image recognition tasks. It is based on the Transformer architecture, which was originally developed for natural language processing tasks. The Vision Transformer uses self-attention mechanisms to process input data and make predictions, and it has been shown to be highly effective for a wide range of image recognition tasks. Some notable applications of Vision Transformers include object detection, image classification, and image generation\n",
        "\n",
        "\n",
        "\n",
        "The ViT model applies the Transformer architecture with self-attention to sequences of image patches, without using convolution layers.\n",
        "\n",
        "![fig](https://miro.medium.com/max/612/1*JmcLTzM1u71_1eq9s71reg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16V_1zhOvgZ4"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rQOa_DF-vgZ5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader,Dataset, TensorDataset\n",
        "from torchvision import transforms\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import tarfile\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6YkMDAFvgZ8"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YR46uRCflfPv",
        "outputId": "56b64c4c-32b2-4565-f962-a231b8402bcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are on GPU !\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  print(\"You are on GPU !\")\n",
        "else:\n",
        "  print('Change the runtime to GPU or continue with CPU, but this should slow down your trainings')\n",
        "  device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kyd-QuQNlScr",
        "outputId": "8bb3dbbe-3134-40c5-9700-ba5967a7244f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-01-11 15:34:24 URL:https://cerfacs.fr/opendata/cours/data/TRAIN.tar [44851200/44851200] -> \"TRAIN.tar\" [1]\n",
            "2024-01-11 15:34:26 URL:https://cerfacs.fr/opendata/cours/data/TEST.tar [15687680/15687680] -> \"TEST.tar\" [1]\n",
            "2024-01-11 15:34:27 URL:https://cerfacs.fr/opendata/cours/data/y_train.npy [1600128/1600128] -> \"y_train.npy\" [1]\n"
          ]
        }
      ],
      "source": [
        "#Loading the dataset\n",
        "!wget -nv https://cerfacs.fr/opendata/cours/data/TRAIN.tar\n",
        "!wget -nv https://cerfacs.fr/opendata/cours/data/TEST.tar\n",
        "!wget -nv https://cerfacs.fr/opendata/cours/data/y_train.npy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TFklZsVWvgZ9"
      },
      "outputs": [],
      "source": [
        "# Using tarfile to create np arrays\n",
        "def extract_files(dataset):\n",
        "    tar = tarfile.open(dataset+'.tar', 'r')\n",
        "    names = tar.getmembers()[:]\n",
        "    images = [tar.extractfile(name) for name in names]\n",
        "    return np.array([np.array(Image.open(image)) for image in images])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUJvWRq_oPL4",
        "outputId": "81fa8ad7-dea5-4b6b-84d0-137d4d7337db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data :  (20000, 64, 64, 3)\n",
            "Train labels (20000, 1)\n",
            "Test data :  (7000, 64, 64, 3)\n"
          ]
        }
      ],
      "source": [
        "X_train = extract_files('TRAIN')\n",
        "X_test = extract_files('TEST')\n",
        "y_train = np.load('y_train.npy').reshape(-1, 1)\n",
        "print(\"Train data : \",X_train.shape)\n",
        "print(\"Train labels\", y_train.shape)\n",
        "print(\"Test data : \",X_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIVcbnuSnw1q"
      },
      "outputs": [],
      "source": [
        "# Display data samples\n",
        "fig, axes = plt.subplots(2, 7, figsize=(15, 4))\n",
        "axes = axes.ravel()\n",
        "for i in range(14):\n",
        "    axes[i].imshow(X_train[i])\n",
        "    axes[i].set_title(y_train[i])\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4qKORKZ7pQ9q"
      },
      "outputs": [],
      "source": [
        "y_train.shape, X_train.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6jlSPn_e0kC"
      },
      "outputs": [],
      "source": [
        "# Convert labels to one-hot encoding\n",
        "encoder = OneHotEncoder()\n",
        "y_train = encoder.fit_transform(y_train).toarray()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "At8waSQUvgZ_"
      },
      "outputs": [],
      "source": [
        "# Prepare data for training\n",
        "X_train, X_test = X_train/255, X_test/255\n",
        "mean = np.mean(X_train, axis=(0, 1, 2))\n",
        "std = np.std(X_train, axis=(0, 1, 2))\n",
        "print(mean)\n",
        "print(std)\n",
        "\n",
        "# Assuming X_train and y_train are PyTorch tensors\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert data to PyTorch tensors and normalize images\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=mean, std=std)\n",
        "])\n",
        "\n",
        "X_train_tensor = torch.stack([transform(img) for img in X_train]).float()\n",
        "X_valid_tensor = torch.stack([transform(img) for img in X_valid]).float()\n",
        "X_test_tensor = torch.stack([transform(img) for img in X_test]).float()\n",
        "y_train_tensor = torch.from_numpy(y_train)\n",
        "y_valid_tensor = torch.from_numpy(y_valid)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnq0ycr0hYZM"
      },
      "outputs": [],
      "source": [
        "X_train_tensor.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKyJGaAPvgaB"
      },
      "source": [
        "## Patches\n",
        "\n",
        "The input to a ViT model is typically an image, which is divided into a grid of patches.\n",
        "\n",
        "Here we implement a patche creator as a layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXWmQNSjvgaB"
      },
      "outputs": [],
      "source": [
        "class Patches(nn.Module):\n",
        "    def __init__(self, patch_size, num_patches=16):\n",
        "        super(Patches, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.patch_dims = patch_size ** 2 * 3\n",
        "        self.num_patches = num_patches\n",
        "\n",
        "    def forward(self, images):\n",
        "        batch_size, channels, height, width = images.size()\n",
        "        # split image into patches\n",
        "        patches = images.unfold(1, channels, channels).unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        patches = patches.contiguous().view(patches.size(0), -1, channels*self.patch_size*self.patch_size)\n",
        "\n",
        "        return patches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmuudVeRvgaF"
      },
      "source": [
        "Let's display patches for a sample image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMYF3NmOvgaH"
      },
      "outputs": [],
      "source": [
        "PATCH_SIZE = 16\n",
        "IMAGE_SIZE = 64\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "\n",
        "# Convert the NumPy image to a PyTorch tensor\n",
        "image = X_train_tensor[np.random.choice(range(X_train.shape[0]))]\n",
        "invTrans = transforms.Compose([ transforms.Normalize(mean = np.zeros_like(mean),\n",
        "                                                     std = 1/std),\n",
        "                                transforms.Normalize(mean = -mean,\n",
        "                                                     std = np.ones_like(std)),\n",
        "                               ])\n",
        "\n",
        "image = invTrans(image)\n",
        "# Create Patches instance\n",
        "patches_layer = Patches(PATCH_SIZE, NUM_PATCHES)\n",
        "\n",
        "# Extract patches\n",
        "patches = patches_layer(image.unsqueeze(0))\n",
        "\n",
        "image_array = torch.swapaxes(image, 2,0).numpy().astype(float)\n",
        "# clip array's values to [0, 1]\n",
        "image_array = np.clip(image_array, 0, 1)\n",
        "# Display the original image\n",
        "plt.figure(figsize=(4, 4))\n",
        "plt.imshow(image_array)\n",
        "plt.axis(\"off\")\n",
        "plt.show()\n",
        "\n",
        "# Display the patches\n",
        "print(f\"Image size: {IMAGE_SIZE} X {IMAGE_SIZE}\")\n",
        "print(f\"Patch size: {PATCH_SIZE} X {PATCH_SIZE}\")\n",
        "print(f\"Patches per image: {patches.shape[1]}\")\n",
        "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
        "\n",
        "n = IMAGE_SIZE // PATCH_SIZE\n",
        "plt.figure(figsize=(4, 4))\n",
        "POSITIONS = []\n",
        "for i in range(n):\n",
        "    for j in range(n):\n",
        "        ax = plt.subplot(n, n, j + n * i + 1)\n",
        "        POSITIONS += [i + n*j]\n",
        "        patch = patches[0,i + n*j,:]\n",
        "        patch_img = patch.view(3, PATCH_SIZE, PATCH_SIZE)\n",
        "        patch_img = np.clip(torch.swapaxes(patch_img, 2,0).numpy().astype(float), 0, 1)\n",
        "        plt.imshow(patch_img)\n",
        "        plt.axis(\"off\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqSwXedovgaM"
      },
      "source": [
        "## The Vision Transformer (ViT) model\n",
        "\n",
        "The ViT model consists of multiple Transformer blocks,\n",
        "which use the `layers.MultiHeadAttention` layer as a self-attention mechanism\n",
        "applied to the sequence of patches. The Transformer blocks produce a\n",
        "`[batch_size, num_patches, projection_dim]` tensor, all the outputs of the final Transformer block are reshaped with `layers.Flatten()` then processed via a classifier head with softmax to produce the final class probabilities output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYTkXShrtdMR"
      },
      "source": [
        "**What is Self-Attention Mechanism ?**\n",
        "\n",
        "Self-attention is a mechanism that allows to a model to process input data by focusing on different parts of the input at different times. It is often used in natural language processing tasks, but can be applied to other types of data, in our case, images.\n",
        "\n",
        "![](https://www.researchgate.net/publication/342301870/figure/fig1/AS:903976534962177@1592536209820/An-example-of-the-self-attention-mechanism-following-long-distance-dependency-in-the.png)\n",
        "\n",
        "Imagine that you are trying to understand a sentence by reading it one word at a time. With self-attention, the model can \"pay attention\" to different words in the sentence at different times, and use this information to better understand the meaning of the sentence. For example, for processing the word \"it\" the model might \"pay attention\" to \"The\" and \"monkey\" more than the other parts of the sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZaiYtKqFqRb"
      },
      "source": [
        "\n",
        "---\n",
        "**Exercise 1**\n",
        "\n",
        "Implement the transformer block of the ViT architecture (the gray block in the figure below).\n",
        "![fig](https://miro.medium.com/max/612/1*JmcLTzM1u71_1eq9s71reg.png)\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "9KUdBzugiKty",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "aa821772248ad3ef4978e7df58cd9b4a",
          "grade": false,
          "grade_id": "cell-db6fc65d10e4d889",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "NUM_HEADS = 8\n",
        "PROJECTION_DIM = 64\n",
        "TRANSFORMER_UNITS = [PROJECTION_DIM * 2, PROJECTION_DIM]\n",
        "NUM_CLASSES = 10\n",
        "PATCH_SIZE = 16\n",
        "NUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n",
        "MLP_HEAD_UNITS = [2048, 1024]  # Size of the dense layers of the final classifier\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.norm1 = nn.LayerNorm(PROJECTION_DIM, eps=1e-6)\n",
        "        self.attention = nn.MultiheadAttention(embed_dim=PROJECTION_DIM, num_heads=NUM_HEADS)\n",
        "        self.norm2 = nn.LayerNorm(PROJECTION_DIM, eps=1e-6)\n",
        "        self.mlp = MLP(PROJECTION_DIM, TRANSFORMER_UNITS)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        return out\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_units, dropout_rate=0.0):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc_layers = nn.ModuleList()\n",
        "        in_features = input_size\n",
        "\n",
        "        for units in hidden_units:\n",
        "            self.fc_layers.append(nn.Linear(in_features, units))\n",
        "            in_features = units\n",
        "        self.dropout = nn.Dropout(dropout_rate) if dropout_rate > 0.0 else None\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.fc_layers:\n",
        "            x = layer(x)\n",
        "            x = F.gelu(x)\n",
        "            if self.dropout:\n",
        "                x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, transformer_layers):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patches_layer = Patches(PATCH_SIZE, NUM_PATCHES)\n",
        "\n",
        "        self.projection = nn.Linear(3*PATCH_SIZE**2, PROJECTION_DIM)\n",
        "        self.positions = torch.arange(NUM_PATCHES).unsqueeze(0).to(device)\n",
        "        self.position_embeddings = nn.Embedding(NUM_PATCHES, PROJECTION_DIM)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock() for _ in range(transformer_layers)])\n",
        "        self.layernorm = nn.LayerNorm(PROJECTION_DIM, eps=1e-6)\n",
        "        self.mlp_head = MLP(NUM_PATCHES*PROJECTION_DIM, MLP_HEAD_UNITS, dropout_rate=0.0)\n",
        "\n",
        "        self.classifier = nn.Linear(MLP_HEAD_UNITS[-1], NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        logits = self.classifier(features)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "LtG4lfgwvgaN",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "284cdb763ad7a56b1db6a879f88b43bb",
          "grade": false,
          "grade_id": "Exercise1",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, transformer_layers):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patches_layer = Patches(PATCH_SIZE, NUM_PATCHES)\n",
        "\n",
        "        self.projection = nn.Linear(3*PATCH_SIZE**2, PROJECTION_DIM)\n",
        "        #self.positions = torch.Tensor(POSITIONS).unsqueeze(0).long().to(device)\n",
        "        self.positions = torch.arange(NUM_PATCHES).unsqueeze(0).to(device)\n",
        "        self.position_embeddings = nn.Embedding(NUM_PATCHES, PROJECTION_DIM)\n",
        "\n",
        "        self.transformer_blocks = nn.ModuleList([TransformerBlock() for _ in range(transformer_layers)])\n",
        "        self.layernorm = nn.LayerNorm(PROJECTION_DIM, eps=1e-6)\n",
        "        self.mlp_head = MLP(NUM_PATCHES*PROJECTION_DIM, MLP_HEAD_UNITS, dropout_rate=0.0)\n",
        "\n",
        "        self.classifier = nn.Linear(MLP_HEAD_UNITS[-1], NUM_CLASSES)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        logits = self.classifier(features)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9BKxwj5179IP"
      },
      "outputs": [],
      "source": [
        "vit_model = VisionTransformer(transformer_layers=1)\n",
        "print(vit_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUrj2kbq2NPh"
      },
      "source": [
        "## A CNN reference\n",
        "\n",
        "Here we implement a CNN as a reference for comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "tMrkmkqv0Q2K",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "1e6e64926be501161a808fe035cbc5ed",
          "grade": false,
          "grade_id": "cell-1fe1fee3a706c2a2",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Define the simple CNN model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        # ... (add more layers as needed)\n",
        "        # self.fc = nn.Linear(..., 10)  # Output has 10 classes\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        # ... (add forward pass for other layers)\n",
        "        # YOUR CODE HERE\n",
        "        raise NotImplementedError()\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4PY25MvvgaO"
      },
      "source": [
        "## Compile, train, and evaluate the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnR_VG-TJQaj"
      },
      "source": [
        "---\n",
        "\n",
        "**Exercise 2**\n",
        "\n",
        "Train the CNN model over `NUM_EPOCHS` epochs using `Adam` optimizer with the given learning rate `LR`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nYUXUXo6JOUJ",
        "tags": []
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_tensor, labels_tensor, transform=None):\n",
        "        self.data_tensor = data_tensor\n",
        "        self.labels_tensor = labels_tensor\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_tensor)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = self.data_tensor[idx]\n",
        "        label = self.labels_tensor[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVT2X2f-6hqs"
      },
      "outputs": [],
      "source": [
        "LR = 3e-4\n",
        "WEIGHT_DECAY = 1e-4\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "transfos = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(degrees=180),\n",
        "    #transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),\n",
        "    #transforms.RandomGrayscale(p=0.1),\n",
        "    #transforms.RandomPerspective(distortion_scale=0.5, p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = CustomDataset(X_train_tensor, y_train_tensor.argmax(dim=1))#, transform=transfos)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "\n",
        "valid_dataset = CustomDataset(X_valid_tensor, y_valid_tensor.argmax(dim=1))\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qj9O0aSVw2d9"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm, notebook\n",
        "\n",
        "def train(model, num_epochs, train_loader, valid_loader,lr=LR, weight_decay=0):\n",
        "    # Define the loss function and optimizer\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # Initialize an empty dictionary to store accuracy\n",
        "    history_dict = {'train_acc': [], 'valid_acc': [], 'train_loss': [], 'valid_loss': []}\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        with notebook.tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs}', unit='batch') as train_bar:\n",
        "            for inputs, labels in train_bar:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "\n",
        "                _, predicted_train = outputs.max(1)\n",
        "                total_train += labels.size(0)\n",
        "                correct_train += predicted_train.eq(labels).sum().item()\n",
        "\n",
        "                # Update tqdm description with training loss\n",
        "                train_bar.set_postfix({'Training Loss': train_loss / (train_bar.n + 1)})\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        valid_loss = 0.0\n",
        "        correct_valid = 0\n",
        "        total_valid = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with tqdm(valid_loader, desc='Validation', unit='batch') as valid_bar:\n",
        "                for inputs, labels in valid_bar:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                    valid_loss += loss.item()\n",
        "\n",
        "                    _, predicted_valid = outputs.max(1)\n",
        "                    total_valid += labels.size(0)\n",
        "                    correct_valid += predicted_valid.eq(labels).sum().item()\n",
        "\n",
        "                    # Update tqdm description with validation loss\n",
        "                    valid_bar.set_postfix({'Validation Loss': valid_loss / (valid_bar.n + 1)})\n",
        "\n",
        "        # Compute accuracy and store in dictionary\n",
        "        accuracy_train = correct_train / total_train\n",
        "        accuracy_valid = correct_valid / total_valid\n",
        "        history_dict['train_acc'].append(accuracy_train)\n",
        "        history_dict['valid_acc'].append(accuracy_valid)\n",
        "        history_dict['train_loss'].append(train_loss / len(train_loader))\n",
        "        history_dict['valid_loss'].append(valid_loss / len(valid_loader))\n",
        "\n",
        "        print(f'Epoch {epoch + 1}/{num_epochs}, '\n",
        "              f'Training Loss: {train_loss / len(train_loader):.3f}, '\n",
        "              f'Validation Loss: {valid_loss / len(valid_loader):.3f}, '\n",
        "              f'Training Accuracy: {100 * accuracy_train:.2f}%, '\n",
        "              f'Validation Accuracy: {100 * accuracy_valid:.2f}%')\n",
        "\n",
        "    return model, history_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xy1pbZ9dDO3u"
      },
      "outputs": [],
      "source": [
        "NUM_FILTERS = 32\n",
        "\n",
        "# Instantiate the model\n",
        "cnn_model = SimpleCNN().to(device)\n",
        "\n",
        "# Print the model architecture\n",
        "print(cnn_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQIHmzFHxj4D"
      },
      "outputs": [],
      "source": [
        "NUM_EPOCHS = 20\n",
        "cnn_model, acc_dict_cnn = train(cnn_model, NUM_EPOCHS, train_loader, valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUES1k6L2Kck"
      },
      "outputs": [],
      "source": [
        "def plot_accuracy(histories = {'CNN':None, 'ViT':None}, metric='acc'):\n",
        "  n = len(histories.keys())\n",
        "  plt.figure(figsize=(25,9))\n",
        "  for j, key in enumerate(histories.keys()):\n",
        "    ax = plt.subplot(1, n, j + 1)\n",
        "    plt.plot(histories[key][f'train_{metric}'],label=f'Train {metric}')\n",
        "    plt.plot(histories[key][f'valid_{metric}'],label=f'Validation {metric}')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.title(key +\" - \" + \"accuracy\")\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx34qsqpJ4fX"
      },
      "outputs": [],
      "source": [
        "plot_accuracy(histories = {'CNN':acc_dict_cnn})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_MnrK13J_CL"
      },
      "source": [
        "There is a clear overfitting, but the model reaches 80% accuracy.\n",
        "\n",
        "---\n",
        "**Exercise 3**\n",
        "\n",
        "Now, train the ViT model using the same configuration.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni7_lY20I5ow",
        "tags": []
      },
      "outputs": [],
      "source": [
        "TRANSFORMER_LAYERS = 4\n",
        "vit_model = VisionTransformer(transformer_layers=TRANSFORMER_LAYERS).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLtPZpdp1IME"
      },
      "outputs": [],
      "source": [
        "vit_model, acc_dict_vit = train(vit_model, NUM_EPOCHS, train_loader, valid_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCJDp_u9LqCH"
      },
      "outputs": [],
      "source": [
        "plot_accuracy(histories = {'ViT':acc_dict_vit})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7A01JpXZOV9S",
        "tags": []
      },
      "source": [
        "Well, there is a significant overfitting happening here.\n",
        "\n",
        "---\n",
        "**Exercise 4**\n",
        "\n",
        "- Add dropout rates to the `MultiHeadAttention` layer and to the MLP block of the transformer block (using a rate of 10% both)\n",
        "- Add a `Dropout` layer just before the last MLP block (with a rate of 50%).\n",
        "- Add dropout rate to the last MLP head block (using a rate of 50%).\n",
        "- Use `Weight decay` in the optimzer.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzFz_4KHUWyu"
      },
      "source": [
        "**Weight decay**\n",
        "\n",
        "Weight Decay, or $L_2$ Regularization, is a regularization technique  encouraging smaller weights. We minimize a loss function compromising both the primary loss function and a penalty on the weights:\n",
        "$$\\mathbf{L}(W) = \\mathbf{L}_{original}(W) + \\lambda ||W||2$$\n",
        "\n",
        "\n",
        "Where $\\lambda$ is a value determining the strength of the penalty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "id": "PJrXDmqWosCK",
        "nbgrader": {
          "cell_type": "code",
          "checksum": "115c8e6771135fc3c9e1dbfbd6e8d709",
          "grade": false,
          "grade_id": "cell-0231153755b40087",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        }
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q2cHgj7PvgaP",
        "tags": []
      },
      "outputs": [],
      "source": [
        "TRANSFORMER_LAYERS = 8\n",
        "vit_model_2 = VisionTransformer(transformer_layers=TRANSFORMER_LAYERS).to(device)\n",
        "vit_model_2, acc_dict_vit_2 = train(vit_model_2, NUM_EPOCHS, train_loader, valid_loader, weight_decay=WEIGHT_DECAY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-MzPRWB8Hc_z"
      },
      "outputs": [],
      "source": [
        "plot_accuracy(histories = {'ViT':acc_dict_vit})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIDTLLQbYKol"
      },
      "source": [
        "We have increased accuracy by 10% and reduced the gap between the training score and the validation score. We're still a long way from CNN's 80% accuracy, but that gives you an idea about ViT's scalability potential."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJAmXoU2ZLAP"
      },
      "source": [
        "Now you can use all the available data to train the ViT. You can also increase the complexity of the ViT by using more transformer layers.\n",
        "\n",
        "Another possibility is to take advantage of the hype around transformers to find a pre-trained model for transfer learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzHxS5Vh8M5k"
      },
      "source": [
        "## Attention rollout\n",
        "To investigate the learned representations, [Abnar et al](https://arxiv.org/abs/2005.00928). introduce \"Attention rollout\" for quantifying how information flow through self-attention layers of Transformer blocks, this is computed by averaging attention weights of a ViT across all heads and multipliying the weight matrices of all layers.\n",
        "\n",
        "We don't implement it here, but here is an example of the resulting attention map of a ViT.\n",
        "\n",
        "![](https://github.com/relmonta/machine-learning/blob/main/source/TP8/attention_rollout.png?raw=1)\n",
        "\n",
        "![](https://user-images.githubusercontent.com/68524289/117996628-454ead00-b37d-11eb-8717-8135f57b7fac.png)\n",
        "\n",
        "Refer to the [Keras doc](https://keras.io/examples/vision/probing_vits/#method-ii-attention-rollout) for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crFhK4LkXD2t"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pNOX5dRZvgZ3"
      ],
      "provenance": []
    },
    "environment": {
      "name": "tf2-gpu.2-4.m61",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-4:m61"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}